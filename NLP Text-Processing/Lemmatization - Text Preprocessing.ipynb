{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet Lemmatizer\n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
    "\n",
    "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma.\n",
    "- Lemmatization is used in Q&A, Chatbots, & Text Summarization\n",
    "- Lemmatization has a dictonary for all of the root words, therefore it is better than stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "'''\n",
    "Types of POS in Lemmatization:  \n",
    "Noun - n\n",
    "verb - v\n",
    "adjective - a\n",
    "adverb - r\n",
    "'''\n",
    "\n",
    "lemmatizer.lemmatize(\"going\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eats --> eat\n",
      "eaten --> eat\n",
      "writing --> write\n",
      "writes --> write\n",
      "programming --> program\n",
      "programs --> program\n",
      "history --> history\n",
      "finally --> finally\n",
      "finalized --> finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \" --> \" + lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words whose stem were not properly found using stemming. Hence, using lemmatization to find the root word\n",
    "lemmatizer.lemmatize(\"fairly\", pos='v'), lemmatizer.lemmatize(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When working with a large set of text, manually setting the pos parameter for each word can be time-consuming and impractical. Here are a few strategies to help you efficiently handle this: \\n\\nUse a POS tagger: Before lemmatizing, use a POS tagger (such as NLTK's pos_tag() function) to automatically assign a POS tag to each word in your text. This way, you can programmatically determine the pos parameter for each word.\\n\\nUse a library with automated POS detection: Some libraries, like spaCy, automatically detect the POS tag for each word when you process the text. You can then use this information to set the pos parameter.\\n\\nUse a default POS: If you don't have a strong requirement for precise POS tagging, you can set a default pos parameter (e.g., 'N' for noun) and apply it to all words. This might not be ideal, but it can save time.\\n\\nPre-process and store POS tags: If you're working with a large, static dataset, you can pre-process the text, store the POS tags alongside the words, and then use this information when lemmatizing.\\nBy implementing one of these strategies, you can efficiently handle setting the pos parameter for each word in your large text dataset.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''When working with a large set of text, manually setting the pos parameter for each word can be time-consuming and impractical. Here are a few strategies to help you efficiently handle this: \\n\n",
    "Use a POS tagger: Before lemmatizing, use a POS tagger (such as NLTK's pos_tag() function) to automatically assign a POS tag to each word in your text. This way, you can programmatically determine the pos parameter for each word.\n",
    "\n",
    "Use a library with automated POS detection: Some libraries, like spaCy, automatically detect the POS tag for each word when you process the text. You can then use this information to set the pos parameter.\n",
    "\n",
    "Use a default POS: If you don't have a strong requirement for precise POS tagging, you can set a default pos parameter (e.g., 'N' for noun) and apply it to all words. This might not be ideal, but it can save time.\n",
    "\n",
    "Pre-process and store POS tags: If you're working with a large, static dataset, you can pre-process the text, store the POS tags alongside the words, and then use this information when lemmatizing.\n",
    "\n",
    "By implementing one of these strategies, you can efficiently handle setting the pos parameter for each word in your large text dataset.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "Lemmatized Tokens: ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize tagged tokens\n",
    "lemmatized_tokens = []\n",
    "for token, pos_tag in tagged_tokens:\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    wn_pos_tag = nltk.corpus.wordnet.VERB if pos_tag.startswith('V') else nltk.corpus.wordnet.NOUN\n",
    "    lemmatized_token = lemmatizer.lemmatize(token, pos=wn_pos_tag)\n",
    "    lemmatized_tokens.append(lemmatized_token)\n",
    "\n",
    "# Print original tokens, POS tags, and lemmatized tokens\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"POS Tags:\", tagged_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
